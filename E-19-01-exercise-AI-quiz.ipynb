{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mature-improvement",
   "metadata": {},
   "source": [
    "# [E19]-(ì—°ìŠµ) ì¸ê°„ë³´ë‹¤ í€´ì¦ˆë¥¼ ì˜í‘¸ëŠ” ì¸ê³µì§€ëŠ¥  \n",
    "  \n",
    "* ì¤€ë¹„ë¬¼\n",
    "ì‹œì‘í•˜ê¸°ì— ì•ì„œ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ í´ë”ë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.  \n",
    "```\n",
    "$ mkdir -p ~/aiffel/bert_qna/data\n",
    "$ mkdir -p ~/aiffel/bert_qna/models\n",
    "```  \n",
    "  \n",
    "ê·¸ë¦¬ê³  í•œêµ­ì–´ ì‹œê°í™”ë¥¼ ìœ„í•´ ì•„ë˜ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ ì¤ë‹ˆë‹¤.  \n",
    "```\n",
    "$ sudo apt update -qq\n",
    "$ sudo apt install fonts-nanum* -qq\n",
    "```  \n",
    "  \n",
    "pydotì„ ì´ìš©í•œ ëª¨ë¸ ì‹œê°í™”ë¥¼ ìœ„í•´ ì•„ë˜ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ ì¤ë‹ˆë‹¤.  \n",
    "```\n",
    "$ sudo apt graphviz\n",
    "```\n",
    "  \n",
    "ì•„ë˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ê³  ë¯¸ì„¤ì¹˜ ì‹œ ì„¤ì¹˜í•´ ì¤ë‹ˆë‹¤.  \n",
    "ì´ë²ˆ ë…¸ë“œì—ì„œëŠ” tensorflowì™€ tensorflow_addonsì˜ ë²„ì „ í˜¸í™˜ì„±ì„ ìœ ì˜í•´ì£¼ì„¸ìš”. (ğŸ’¡tensorflow 2.2.0, tensorflow_addons 0.11.2ë¥¼ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.)  \n",
    "```\n",
    "$ pip install tensorflow_addons==0.11.2\n",
    "$ pip install sentencepiece\n",
    "$ pip install wordcloud\n",
    "$ pip install ipywidgets --user\n",
    "$ pip install tqdm\n",
    "$ pip install pydot\n",
    "$ pip install pydotplus\n",
    "$ pip install graphviz\n",
    "```  \n",
    "\n",
    "í˜¹ì‹œ ipywidgetsì´ë‚˜ pydotì´ ìƒˆë¡œ ì„¤ì¹˜ë˜ì—ˆë‹¤ë©´ ì»¤ë„ì„ ì¢…ë£Œ í›„ ì¬ì‹œì‘í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ í°íŠ¸ ì„¤ì¹˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-testament",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ  \n",
    "ë¨¼ì € ì•„ë˜ì™€ ê°™ì´ KorQuAD 1.0 ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.  \n",
    "```\n",
    "$ wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
    "$ wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n",
    "$ mv KorQuAD_v1.0* ~/aiffel/bert_qna/data\n",
    "```  \n",
    "ì´ì™¸ì—, ì˜¤ëŠ˜ ìš°ë¦¬ê°€ ì‹¤í—˜í•´ì„œ í™œìš©í•  model, vocab, text corpus ë°ì´í„° ë“±ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œë°›ì•„ ë‘¡ì‹œë‹¤.  \n",
    "```\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/ko_32000.model\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/ko_32000.vocab\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/bert_pretrain_32000.hdf5\n",
    "$ mv ko_32000* ~/aiffel/bert_qna/models\n",
    "$ mv bert_pretrain_32000.hdf5 ~/aiffel/bert_qna/models\n",
    "```\n",
    "  \n",
    "```\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/kowiki.txt.zip\n",
    "$ mv kowiki.txt.zip ~/aiffel/bert_qna/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_json_tree(data, indent=\"\"):\n",
    "    for key, value in data.items():\n",
    "        if type(value) == list:     # list í˜•íƒœì˜ itemì€ ì²«ë²ˆì§¸ itemë§Œ ì¶œë ¥\n",
    "            print(f'{indent}- {key}: [{len(value)}]')\n",
    "            print_json_tree(value[0], indent + \"  \")\n",
    "        else:\n",
    "            print(f'{indent}- {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_qna/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_qna/models'\n",
    "\n",
    "# í›ˆë ¨ë°ì´í„° í™•ì¸\n",
    "train_json_path = data_dir + '/KorQuAD_v1.0_train.json'\n",
    "with open(train_json_path) as f:\n",
    "    train_json = json.load(f)\n",
    "    print_json_tree(train_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ì¦ë°ì´í„° í™•ì¸\n",
    "dev_json_path = data_dir + '/KorQuAD_v1.0_dev.json'\n",
    "with open(dev_json_path) as f:\n",
    "    dev_json = json.load(f)\n",
    "    print_json_tree(dev_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(train_json[\"data\"][0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-harvard",
   "metadata": {},
   "source": [
    "## 2. KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-receipt",
   "metadata": {},
   "source": [
    "### KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (1) ë„ì–´ì“°ê¸° ë‹¨ìœ„ ì •ë³´ê´€ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitespaceê°€ 2ê°œì¸ ê²½ìš°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•¨\n",
    "\n",
    "string1 = '1839ë…„ íŒŒìš°ìŠ¤íŠ¸ì„ ì½ì—ˆë‹¤.'\n",
    "string2 = '1839ë…„  íŒŒìš°ìŠ¤íŠ¸ì„ ì½ì—ˆë‹¤.'\n",
    "string1[6:10], string2[7:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = []\n",
    "char_to_word = []\n",
    "prev_is_whitespace = True\n",
    "\n",
    "# ì²«ë²ˆì§¸ ë¬¸ì¥(string1)ì— ëŒ€í•´ ë„ì–´ì“°ê¸° ì˜ì—­ ì •ë³´ë¥¼ í‘œì‹œ\n",
    "for c in string1:\n",
    "    if _is_whitespace(c):\n",
    "        prev_is_whitespace = True\n",
    "    else:\n",
    "        if prev_is_whitespace:\n",
    "            word_tokens.append(c)\n",
    "        else:\n",
    "            word_tokens[-1] += c\n",
    "        prev_is_whitespace = False    \n",
    "    char_to_word.append(len(word_tokens) - 1)\n",
    "    print(f'\\'{c}\\' : {word_tokens} : {char_to_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = []\n",
    "char_to_word = []\n",
    "prev_is_whitespace = True\n",
    "\n",
    "# ë‘ë²ˆì§¸ ë¬¸ì¥(string2)ì— ëŒ€í•´ ë„ì–´ì“°ê¸° ì˜ì—­ ì •ë³´ë¥¼ í‘œì‹œ\n",
    "for c in string2:\n",
    "    if _is_whitespace(c):\n",
    "        prev_is_whitespace = True\n",
    "    else:\n",
    "        if prev_is_whitespace:\n",
    "            word_tokens.append(c)\n",
    "        else:\n",
    "            word_tokens[-1] += c\n",
    "        prev_is_whitespace = False    \n",
    "    char_to_word.append(len(word_tokens) - 1)\n",
    "    print(f'\\'{c}\\' : {word_tokens} : {char_to_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_whitespace(string):\n",
    "    word_tokens = []\n",
    "    char_to_word = []\n",
    "    prev_is_whitespace = True\n",
    "\n",
    "    for c in string:\n",
    "        if _is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "        else:\n",
    "            if prev_is_whitespace:\n",
    "                word_tokens.append(c)\n",
    "            else:\n",
    "                word_tokens[-1] += c\n",
    "            prev_is_whitespace = False    \n",
    "        char_to_word.append(len(word_tokens) - 1)\n",
    "    \n",
    "    return word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²«ë²ˆì§¸ ë¬¸ì¥(string1)ì— ëŒ€í•´ ë„ì–´ì“°ê¸° ì˜ì—­ ì •ë³´ë¥¼ í‘œì‹œ\n",
    "word_tokens, char_to_word = _tokenize_whitespace(string1)\n",
    "for c, i in zip(list(string1), char_to_word):\n",
    "    print(f'\\'{c}\\' : {i}')\n",
    "\n",
    "word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ë²ˆì§¸ ë¬¸ì¥(string2)ì— ëŒ€í•´ ë„ì–´ì“°ê¸° ì˜ì—­ ì •ë³´ë¥¼ í‘œì‹œ\n",
    "word_tokens, char_to_word = _tokenize_whitespace(string2)\n",
    "for c, i in zip(list(string2), char_to_word):\n",
    "    print(f'\\'{c}\\' : {i}')\n",
    "\n",
    "word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-regulation",
   "metadata": {},
   "source": [
    "### KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (2) Tokenize by Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")\n",
    "\n",
    "# wordë¥¼ subwordë¡œ ë³€ê²½í•˜ë©´ì„œ index ì €ì¥\n",
    "word_to_token = []\n",
    "context_tokens = []\n",
    "for (i, word) in enumerate(word_tokens):\n",
    "    word_to_token.append(len(context_tokens))\n",
    "    tokens = vocab.encode_as_pieces(word)  # SentencePieceë¥¼ ì‚¬ìš©í•´ Subwordë¡œ ìª¼ê°­ë‹ˆë‹¤.\n",
    "    for token in tokens:\n",
    "        context_tokens.append(token)\n",
    "\n",
    "context_tokens, word_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_vocab(vocab, context_words):\n",
    "    word_to_token = []\n",
    "    context_tokens = []\n",
    "    for (i, word) in enumerate(context_words):\n",
    "        word_to_token.append(len(context_tokens))\n",
    "        tokens = vocab.encode_as_pieces(word)\n",
    "        for token in tokens:\n",
    "            context_tokens.append(token)\n",
    "    return context_tokens, word_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokens)  # ì²˜ë¦¬í•´ì•¼ í•  word ë‹¨ìœ„ ì…ë ¥\n",
    "\n",
    "context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)\n",
    "context_tokens, word_to_token   # Subword ë‹¨ìœ„ë¡œ í† í°í™”í•œ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-kitchen",
   "metadata": {},
   "source": [
    "### KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (3) Improve Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = train_json['data'][0]['paragraphs'][0]['context']\n",
    "question = train_json['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "answer_text = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text']\n",
    "answer_start = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
    "answer_end = answer_start + len(answer_text) - 1\n",
    "\n",
    "print('[context] ', context)\n",
    "print('[question] ', question)\n",
    "print('[answer] ', answer_text)\n",
    "print('[answer_start] index: ', answer_start, 'character: ', context[answer_start])\n",
    "print('[answer_end]index: ', answer_end, 'character: ', context[answer_end])\n",
    "\n",
    "# answer_textì— í•´ë‹¹í•˜ëŠ” context ì˜ì—­ì„ ì •í™•íˆ ì°¾ì•„ë‚´ì•¼ í•©ë‹ˆë‹¤. \n",
    "assert context[answer_start:answer_end + 1] == answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextë¥¼ ë„ì–´ì“°ê¸°(word) ë‹¨ìœ„ë¡œ í† í°í™”í•œ ê²°ê³¼ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤. \n",
    "word_tokens, char_to_word = _tokenize_whitespace(context)\n",
    "\n",
    "print( word_tokens[:20])\n",
    "\n",
    "char_to_word[:20], context[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„ì–´ì“°ê¸°(word) ë‹¨ìœ„ë¡œ ìª¼ê°œì§„ context(word_tokens)ë¥¼ Subwordë¡œ í† í°í™”í•œ ê²°ê³¼ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤. \n",
    "context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)\n",
    "for i in range(min(20, len(word_to_token) - 1)):\n",
    "    print(word_to_token[i], context_tokens[word_to_token[i]:word_to_token[i + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_startì™€ answer_endë¡œë¶€í„° word_startì™€ word_endë¥¼ êµ¬í•©ë‹ˆë‹¤. \n",
    "word_start = char_to_word[answer_start]\n",
    "word_end = char_to_word[answer_end]\n",
    "word_start, word_end, answer_text, word_tokens[word_start:word_end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_start = word_to_token[word_start]\n",
    "if word_end < len(word_to_token) - 1:\n",
    "    token_end = word_to_token[word_end + 1] - 1\n",
    "else:\n",
    "    token_end = len(context_tokens) - 1\n",
    "token_start, token_end, context_tokens[token_start:token_end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì œ ì •ë‹µì¸ answer_textë„ Subword ê¸°ì¤€ìœ¼ë¡œ í† í°í™”í•´ ë‘¡ë‹ˆë‹¤. \n",
    "token_answer = \" \".join(vocab.encode_as_pieces(answer_text))\n",
    "token_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ë‹µì´ ë ìˆ˜ ìˆëŠ” new_startì™€ new_endì˜ ê²½ìš°ë¥¼ ìˆœíšŒíƒìƒ‰í•©ë‹ˆë‹¤. \n",
    "for new_start in range(token_start, token_end + 1):\n",
    "    for new_end in range(token_end, new_start - 1, -1):\n",
    "        text_span = \" \".join(context_tokens[new_start : (new_end + 1)])\n",
    "        if text_span == token_answer:   # ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš°\n",
    "            print(\"O >>\", (new_start, new_end), text_span)\n",
    "        else:\n",
    "            print(\"X >>\", (new_start, new_end), text_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_tokensì—ì„œ char_answerì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ ë¦¬í„´í•˜ëŠ” í•¨ìˆ˜\n",
    "def _improve_span(vocab, context_tokens, token_start, token_end, char_answer):\n",
    "    token_answer = \" \".join(vocab.encode_as_pieces(char_answer))\n",
    "    for new_start in range(token_start, token_end + 1):\n",
    "        for new_end in range(token_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(context_tokens[new_start : (new_end + 1)])\n",
    "            if text_span == token_answer:\n",
    "                return (new_start, new_end)\n",
    "    return (token_start, token_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, answer_text)\n",
    "print('token_start:', token_start, ' token_end:', token_end)\n",
    "context_tokens[token_start:token_end + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-lloyd",
   "metadata": {},
   "source": [
    "### KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (4) ë°ì´í„°ì…‹ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_korquad(vocab, json_data, out_file):\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for data in tqdm(json_data[\"data\"]):\n",
    "            title = data[\"title\"]\n",
    "            for paragraph in data[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]\n",
    "                context_words, char_to_word = _tokenize_whitespace(context)\n",
    "\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    assert len(qa[\"answers\"]) == 1\n",
    "                    qa_id = qa[\"id\"]\n",
    "                    question = qa[\"question\"]\n",
    "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                    answer_start = qa[\"answers\"][0][\"answer_start\"]\n",
    "                    answer_end = answer_start + len(answer_text) - 1\n",
    "\n",
    "                    assert answer_text == context[answer_start:answer_end + 1]\n",
    "\n",
    "                    word_start = char_to_word[answer_start]\n",
    "                    word_end = char_to_word[answer_end]\n",
    "\n",
    "                    word_answer = \" \".join(context_words[word_start:word_end + 1])\n",
    "                    char_answer = \" \".join(answer_text.strip().split())\n",
    "                    assert char_answer in word_answer\n",
    "\n",
    "                    context_tokens, word_to_token = _tokenize_vocab(vocab, context_words)\n",
    "\n",
    "                    token_start = word_to_token[word_start]\n",
    "                    if word_end < len(word_to_token) - 1:\n",
    "                        token_end = word_to_token[word_end + 1] - 1\n",
    "                    else:\n",
    "                        token_end = len(context_tokens) - 1\n",
    "\n",
    "                    token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, char_answer)\n",
    "\n",
    "                    data = {\"qa_id\": qa_id, \"title\": title, \"question\": vocab.encode_as_pieces(question), \"context\": context_tokens, \"answer\": char_answer, \"token_start\": token_start, \"token_end\":token_end}\n",
    "                    f.write(json.dumps(data, ensure_ascii=False))\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ íŒŒì¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "dump_korquad(vocab, train_json, f\"{data_dir}/korquad_train.json\")\n",
    "dump_korquad(vocab, dev_json, f\"{data_dir}/korquad_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_file(filename, count=10):\n",
    "    \"\"\"\n",
    "    íŒŒì¼ ë‚´ìš© ì¶œë ¥\n",
    "    :param filename: íŒŒì¼ ì´ë¦„\n",
    "    :param count: ì¶œë ¥ ë¼ì¸ ìˆ˜\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if count <= i:\n",
    "                break\n",
    "            print(line.strip())\n",
    "\n",
    "print_file(f\"{data_dir}/korquad_train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-harvey",
   "metadata": {},
   "source": [
    "### KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (5) ë°ì´í„° ë¶„ì„ : Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "contexts = []\n",
    "token_starts = []\n",
    "with open(f\"{data_dir}/korquad_train.json\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        questions.append(data[\"question\"])\n",
    "        contexts.append(data[\"context\"])\n",
    "        token_starts.append(data[\"token_start\"])\n",
    "        if i < 10:\n",
    "            print(data[\"token_start\"], data[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token count\n",
    "train_question_counts = [len(question) for question in questions]\n",
    "train_question_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ì— ëŒ€í•œ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ì„ ì–¸\n",
    "# figsize: (ê°€ë¡œ, ì„¸ë¡œ) í˜•íƒœì˜ íŠœí”Œë¡œ ì…ë ¥\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram ì„ ì–¸\n",
    "# bins: íˆìŠ¤í† ê·¸ë¨ ê°’ë“¤ì— ëŒ€í•œ ë²„ì¼“ ë²”ìœ„, \n",
    "# range: xì¶• ê°’ì˜ ë²”ìœ„\n",
    "# facecolor: ê·¸ë˜í”„ ìƒ‰ìƒ\n",
    "# label: ê·¸ë˜í”„ì— ëŒ€í•œ ë¼ë²¨\n",
    "plt.hist(train_question_counts, bins=100, range=[0, 100], facecolor='b', label='train')\n",
    "# ê·¸ë˜í”„ ì œëª©\n",
    "plt.title('Count of question')\n",
    "# ê·¸ë˜í”„ x ì¶• ë¼ë²¨\n",
    "plt.xlabel('Number of question')\n",
    "# ê·¸ë˜í”„ y ì¶• ë¼ë²¨\n",
    "plt.ylabel('Count of question')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê¸¸ì´\n",
    "print(f\"question ê¸¸ì´ ìµœëŒ€:    {np.max(train_question_counts):4d}\")\n",
    "print(f\"question ê¸¸ì´ ìµœì†Œ:    {np.min(train_question_counts):4d}\")\n",
    "print(f\"question ê¸¸ì´ í‰ê· :    {np.mean(train_question_counts):7.2f}\")\n",
    "print(f\"question ê¸¸ì´ í‘œì¤€í¸ì°¨: {np.std(train_question_counts):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# ë°±ë¶„ìœ„ìˆ˜(Percentile)ëŠ” í¬ê¸°ê°€ ìˆëŠ” ê°’ë“¤ë¡œ ì´ë¤„ì§„ ìë£Œë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í–ˆì„ ë•Œ ë°±ë¶„ìœ¨ë¡œ ë‚˜íƒ€ë‚¸ íŠ¹ì • ìœ„ì¹˜ì˜ ê°’ì„ ì´ë¥´ëŠ” ìš©ì–´ì´ë‹¤.\n",
    "# ì¼ë°˜ì ìœ¼ë¡œ í¬ê¸°ê°€ ì‘ì€ ê²ƒë¶€í„° ë‚˜ì—´í•˜ì—¬ ê°€ì¥ ì‘ì€ ê²ƒì„ 0, ê°€ì¥ í° ê²ƒì„ 100ìœ¼ë¡œ í•œë‹¤.\n",
    "# 100ê°œì˜ ê°’ì„ ê°€ì§„ ì–´ë–¤ ìë£Œì˜ 20 ë°±ë¶„ìœ„ìˆ˜ëŠ” ê·¸ ìë£Œì˜ ê°’ë“¤ ì¤‘ 20ë²ˆì§¸ë¡œ ì‘ì€ ê°’ì„ ëœ»í•œë‹¤. 50 ë°±ë¶„ìœ„ìˆ˜ëŠ” ì¤‘ì•™ê°’ê³¼ ê°™ë‹¤.\n",
    "percentile25 = np.percentile(train_question_counts, 25)\n",
    "percentile50 = np.percentile(train_question_counts, 50)\n",
    "percentile75 = np.percentile(train_question_counts, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"question 25/100ë¶„ìœ„:  {percentile25:7.2f}\")\n",
    "print(f\"question 50/100ë¶„ìœ„:  {percentile50:7.2f}\")\n",
    "print(f\"question 75/100ë¶„ìœ„:  {percentile75:7.2f}\")\n",
    "print(f\"question IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"question MAX/100ë¶„ìœ„: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# ë°•ìŠ¤í”Œë¡¯ ìƒì„±\n",
    "# ì²«ë²ˆì§¸ íŒŒë¼ë©”í„°: ì—¬ëŸ¬ ë¶„í¬ì— ëŒ€í•œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼\n",
    "# labels: ì…ë ¥í•œ ë°ì´í„°ì— ëŒ€í•œ ë¼ë²¨\n",
    "# showmeans: í‰ê· ê°’ì„ í‘œí˜„\n",
    "# ì°¸ê³ : https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(train_question_counts, labels=['token counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-occasion",
   "metadata": {},
   "source": [
    "### KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (6) ë°ì´í„° ë¶„ì„ : Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token count\n",
    "train_context_counts = [len(context) for context in contexts]\n",
    "train_context_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ì— ëŒ€í•œ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ì„ ì–¸\n",
    "# figsize: (ê°€ë¡œ, ì„¸ë¡œ) í˜•íƒœì˜ íŠœí”Œë¡œ ì…ë ¥\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram ì„ ì–¸\n",
    "# bins: íˆìŠ¤í† ê·¸ë¨ ê°’ë“¤ì— ëŒ€í•œ ë²„ì¼“ ë²”ìœ„, \n",
    "# range: xì¶• ê°’ì˜ ë²”ìœ„\n",
    "# facecolor: ê·¸ë˜í”„ ìƒ‰ìƒ\n",
    "# label: ê·¸ë˜í”„ì— ëŒ€í•œ ë¼ë²¨\n",
    "plt.hist(train_context_counts, bins=900, range=[100, 1000], facecolor='r', label='train')\n",
    "# ê·¸ë˜í”„ ì œëª©\n",
    "plt.title('Count of context')\n",
    "# ê·¸ë˜í”„ x ì¶• ë¼ë²¨\n",
    "plt.xlabel('Number of context')\n",
    "# ê·¸ë˜í”„ y ì¶• ë¼ë²¨\n",
    "plt.ylabel('Count of context')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê¸¸ì´\n",
    "print(f\"context ê¸¸ì´ ìµœëŒ€:    {np.max(train_context_counts):4d}\")\n",
    "print(f\"context ê¸¸ì´ ìµœì†Œ:    {np.min(train_context_counts):4d}\")\n",
    "print(f\"context ê¸¸ì´ í‰ê· :    {np.mean(train_context_counts):7.2f}\")\n",
    "print(f\"context ê¸¸ì´ í‘œì¤€í¸ì°¨: {np.std(train_context_counts):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# ë°±ë¶„ìœ„ìˆ˜(Percentile)ëŠ” í¬ê¸°ê°€ ìˆëŠ” ê°’ë“¤ë¡œ ì´ë¤„ì§„ ìë£Œë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í–ˆì„ ë•Œ ë°±ë¶„ìœ¨ë¡œ ë‚˜íƒ€ë‚¸ íŠ¹ì • ìœ„ì¹˜ì˜ ê°’ì„ ì´ë¥´ëŠ” ìš©ì–´ì´ë‹¤.\n",
    "# ì¼ë°˜ì ìœ¼ë¡œ í¬ê¸°ê°€ ì‘ì€ ê²ƒë¶€í„° ë‚˜ì—´í•˜ì—¬ ê°€ì¥ ì‘ì€ ê²ƒì„ 0, ê°€ì¥ í° ê²ƒì„ 100ìœ¼ë¡œ í•œë‹¤.\n",
    "# 100ê°œì˜ ê°’ì„ ê°€ì§„ ì–´ë–¤ ìë£Œì˜ 20 ë°±ë¶„ìœ„ìˆ˜ëŠ” ê·¸ ìë£Œì˜ ê°’ë“¤ ì¤‘ 20ë²ˆì§¸ë¡œ ì‘ì€ ê°’ì„ ëœ»í•œë‹¤. 50 ë°±ë¶„ìœ„ìˆ˜ëŠ” ì¤‘ì•™ê°’ê³¼ ê°™ë‹¤.\n",
    "percentile25 = np.percentile(train_context_counts, 25)\n",
    "percentile50 = np.percentile(train_context_counts, 50)\n",
    "percentile75 = np.percentile(train_context_counts, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"context 25/100ë¶„ìœ„:  {percentile25:7.2f}\")\n",
    "print(f\"context 50/100ë¶„ìœ„:  {percentile50:7.2f}\")\n",
    "print(f\"context 75/100ë¶„ìœ„:  {percentile75:7.2f}\")\n",
    "print(f\"context IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"context MAX/100ë¶„ìœ„: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# ë°•ìŠ¤í”Œë¡¯ ìƒì„±\n",
    "# ì²«ë²ˆì§¸ íŒŒë¼ë©”í„°: ì—¬ëŸ¬ ë¶„í¬ì— ëŒ€í•œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼\n",
    "# labels: ì…ë ¥í•œ ë°ì´í„°ì— ëŒ€í•œ ë¼ë²¨\n",
    "# showmeans: í‰ê· ê°’ì„ í‘œí˜„\n",
    "# ì°¸ê³ : https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(train_context_counts, labels=['token counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-movie",
   "metadata": {},
   "source": [
    "### KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (7) ë°ì´í„° ë¶„ì„ : Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token count\n",
    "train_answer_starts = token_starts\n",
    "train_answer_starts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ì— ëŒ€í•œ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ì„ ì–¸\n",
    "# figsize: (ê°€ë¡œ, ì„¸ë¡œ) í˜•íƒœì˜ íŠœí”Œë¡œ ì…ë ¥\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram ì„ ì–¸\n",
    "# bins: íˆìŠ¤í† ê·¸ë¨ ê°’ë“¤ì— ëŒ€í•œ ë²„ì¼“ ë²”ìœ„, \n",
    "# range: xì¶• ê°’ì˜ ë²”ìœ„\n",
    "# facecolor: ê·¸ë˜í”„ ìƒ‰ìƒ\n",
    "# label: ê·¸ë˜í”„ì— ëŒ€í•œ ë¼ë²¨\n",
    "plt.hist(train_answer_starts, bins=500, range=[0, 500], facecolor='g', label='train')\n",
    "# ê·¸ë˜í”„ ì œëª©\n",
    "plt.title('Count of answer')\n",
    "# ê·¸ë˜í”„ x ì¶• ë¼ë²¨\n",
    "plt.xlabel('Number of answer')\n",
    "# ê·¸ë˜í”„ y ì¶• ë¼ë²¨\n",
    "plt.ylabel('Count of answer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê¸¸ì´\n",
    "print(f\"answer ìœ„ì¹˜ ìµœëŒ€:    {np.max(train_answer_starts):4d}\")\n",
    "print(f\"answer ìœ„ì¹˜ ìµœì†Œ:    {np.min(train_answer_starts):4d}\")\n",
    "print(f\"answer ìœ„ì¹˜ í‰ê· :    {np.mean(train_answer_starts):7.2f}\")\n",
    "print(f\"answer ìœ„ì¹˜ í‘œì¤€í¸ì°¨: {np.std(train_answer_starts):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# ë°±ë¶„ìœ„ìˆ˜(Percentile)ëŠ” í¬ê¸°ê°€ ìˆëŠ” ê°’ë“¤ë¡œ ì´ë¤„ì§„ ìë£Œë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í–ˆì„ ë•Œ ë°±ë¶„ìœ¨ë¡œ ë‚˜íƒ€ë‚¸ íŠ¹ì • ìœ„ì¹˜ì˜ ê°’ì„ ì´ë¥´ëŠ” ìš©ì–´ì´ë‹¤.\n",
    "# ì¼ë°˜ì ìœ¼ë¡œ í¬ê¸°ê°€ ì‘ì€ ê²ƒë¶€í„° ë‚˜ì—´í•˜ì—¬ ê°€ì¥ ì‘ì€ ê²ƒì„ 0, ê°€ì¥ í° ê²ƒì„ 100ìœ¼ë¡œ í•œë‹¤.\n",
    "# 100ê°œì˜ ê°’ì„ ê°€ì§„ ì–´ë–¤ ìë£Œì˜ 20 ë°±ë¶„ìœ„ìˆ˜ëŠ” ê·¸ ìë£Œì˜ ê°’ë“¤ ì¤‘ 20ë²ˆì§¸ë¡œ ì‘ì€ ê°’ì„ ëœ»í•œë‹¤. 50 ë°±ë¶„ìœ„ìˆ˜ëŠ” ì¤‘ì•™ê°’ê³¼ ê°™ë‹¤.\n",
    "percentile25 = np.percentile(train_answer_starts, 25)\n",
    "percentile50 = np.percentile(train_answer_starts, 50)\n",
    "percentile75 = np.percentile(train_answer_starts, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"answer 25/100ë¶„ìœ„:  {percentile25:7.2f}\")\n",
    "print(f\"answer 50/100ë¶„ìœ„:  {percentile50:7.2f}\")\n",
    "print(f\"answer 75/100ë¶„ìœ„:  {percentile75:7.2f}\")\n",
    "print(f\"answer IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"answer MAX/100ë¶„ìœ„: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# ë°•ìŠ¤í”Œë¡¯ ìƒì„±\n",
    "# ì²«ë²ˆì§¸ íŒŒë¼ë©”í„°: ì—¬ëŸ¬ ë¶„í¬ì— ëŒ€í•œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼\n",
    "# labels: ì…ë ¥í•œ ë°ì´í„°ì— ëŒ€í•œ ë¼ë²¨\n",
    "# showmeans: í‰ê· ê°’ì„ í‘œí˜„\n",
    "# ì°¸ê³ : https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(train_answer_starts, labels=['token counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-collins",
   "metadata": {},
   "source": [
    "### KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (8) ë°ì´í„° ë¶„ì„ : Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train documents\n",
    "documents = []\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ì—ì„œ title, context, question ë¬¸ì¥ì„ ëª¨ë‘ ì¶”ì¶œí•©ë‹ˆë‹¤. \n",
    "for data in tqdm(train_json[\"data\"]):\n",
    "    title = data[\"title\"]\n",
    "    documents.append(title)\n",
    "    for paragraph in data[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        documents.append(context)\n",
    "\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            assert len(qa[\"answers\"]) == 1\n",
    "            question = qa[\"question\"]\n",
    "            documents.append(question)\n",
    "\n",
    "documents[:10]   # ê·¸ì¤‘ ë§¨ ì• 10ê°œë§Œ í™•ì¸í•´ ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentsë¥¼ ì „ë¶€ ì´ì–´ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ë©´ ì´ë ‡ê²Œ ë³´ì…ë‹ˆë‹¤. \n",
    "\" \".join(documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloudë¡œ \" \".join(documents)ë¥¼ ì²˜ë¦¬í•´ ë´…ë‹ˆë‹¤. \n",
    "wordcloud = WordCloud(width=800, height=800, font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(\" \".join(documents))\n",
    "plt.figure(figsize=(10, 10))\n",
    "# image ì¶œë ¥, interpolation ì´ë¯¸ì§€ ì‹œê°í™” ì˜µì…˜\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-welsh",
   "metadata": {},
   "source": [
    "### KorQuAD ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (9) ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = os.path.join(data_dir, \"korquad_train.json\")\n",
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    jsonì„ config í˜•íƒœë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "args = Config({\n",
    "    'max_seq_length': 384,\n",
    "    'max_query_length': 64,\n",
    "})\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„±í•œ ë°ì´í„°ì…‹ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë¡œë”©í•˜ëŠ” í•¨ìˆ˜\n",
    "def load_data(args, filename):\n",
    "    inputs, segments, labels_start, labels_end = [], [], [], []\n",
    "\n",
    "    n_discard = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Loading ...\")):\n",
    "            data = json.loads(line)\n",
    "            token_start = data.get(\"token_start\")\n",
    "            token_end = data.get(\"token_end\")\n",
    "            question = data[\"question\"][:args.max_query_length]\n",
    "            context = data[\"context\"]\n",
    "            answer_tokens = \" \".join(context[token_start:token_end + 1])\n",
    "            context_len = args.max_seq_length - len(question) - 3\n",
    "\n",
    "            if token_end >= context_len:\n",
    "                # ìµœëŒ€ ê¸¸ì´ë‚´ì— tokenì´ ë“¤ì–´ê°€ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ\n",
    "                n_discard += 1\n",
    "                continue\n",
    "            context = context[:context_len]\n",
    "            assert len(question) + len(context) <= args.max_seq_length - 3\n",
    "\n",
    "            tokens = ['[CLS]'] + question + ['[SEP]'] + context + ['[SEP]']\n",
    "            ids = [vocab.piece_to_id(token) for token in tokens]\n",
    "            ids += [0] * (args.max_seq_length - len(ids))\n",
    "            inputs.append(ids)\n",
    "            segs = [0] * (len(question) + 2) + [1] * (len(context) + 1)\n",
    "            segs += [0] * (args.max_seq_length - len(segs))\n",
    "            segments.append(segs)\n",
    "            token_start += (len(question) + 2)\n",
    "            labels_start.append(token_start)\n",
    "            token_end += (len(question) + 2)\n",
    "            labels_end.append(token_end)\n",
    "    print(f'n_discard: {n_discard}')\n",
    "\n",
    "    return (np.array(inputs), np.array(segments)), (np.array(labels_start), np.array(labels_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data load\n",
    "train_inputs, train_labels = load_data(args, train_json)\n",
    "print(f\"train_inputs: {train_inputs[0].shape}\")\n",
    "print(f\"train_inputs: {train_inputs[1].shape}\")\n",
    "print(f\"train_labels: {train_labels[0].shape}\")\n",
    "print(f\"train_labels: {train_labels[1].shape}\")\n",
    "\n",
    "# dev data load\n",
    "dev_inputs, dev_labels = load_data(args, dev_json)\n",
    "print(f\"dev_inputs: {dev_inputs[0].shape}\")\n",
    "print(f\"dev_inputs: {dev_inputs[1].shape}\")\n",
    "print(f\"dev_labels: {dev_labels[0].shape}\")\n",
    "print(f\"dev_labels: {dev_labels[1].shape}\")\n",
    "\n",
    "train_inputs[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questionê³¼ Contextê°€ í¬í•¨ëœ ì…ë ¥ë°ì´í„° 1ë²ˆì§¸\n",
    "train_inputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questionì„ 0ìœ¼ë¡œ, Contextë¥¼ 1ë¡œ êµ¬ë¶„í•´ ì¤€ Segment ë°ì´í„° 1ë²ˆì§¸\n",
    "train_inputs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answerìœ„ì¹˜ì˜ ì‹œì‘ì ê³¼ ëì  ë¼ë²¨ 1ë²ˆì§¸\n",
    "train_labels[0][0], train_labels[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-insider",
   "metadata": {},
   "source": [
    "## 3. LSTMì„ ì´ìš©í•œ ë„ì „"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lstm(n_vocab, n_seq, d_model):\n",
    "    tokens = tf.keras.layers.Input((None,), name='tokens')\n",
    "    segments = tf.keras.layers.Input((None,), name='segments')\n",
    "\n",
    "    hidden = tf.keras.layers.Embedding(n_vocab, d_model)(tokens) + tf.keras.layers.Embedding(2, d_model)(segments) # (bs, n_seq, d_model)\n",
    "\n",
    "    hidden = tf.keras.layers.LSTM(d_model, return_sequences=True)(hidden)  # (bs, n_seq, d_model)\n",
    "    hidden = tf.keras.layers.LSTM(d_model, return_sequences=True)(hidden)  # (bs, n_seq, d_model)\n",
    "    hidden = tf.keras.layers.Dense(2)(hidden) # (bs, n_seq, 2)\n",
    "    start_logits, end_logits = tf.split(hidden, 2, axis=-1)  # (bs, n_seq, 1), (bs, n_seq, 1)\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1)  # (bs, n_seq)\n",
    "    start_outputs = tf.keras.layers.Softmax(name=\"start\")(start_logits)\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1)  # (bs, n_seq)\n",
    "    end_outputs = tf.keras.layers.Softmax(name=\"end\")(end_logits)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(tokens, segments), outputs=(start_outputs, end_outputs))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_lstm(n_vocab=len(vocab), n_seq=512, d_model=512)\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_start_accuracy', patience=5)\n",
    "# save weights\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(data_dir, \"korquad_lstm.hdf5\"), monitor='val_start_accuracy', verbose=1, save_best_only=True, mode='max', save_freq='epoch', save_weights_only=True)\n",
    "\n",
    "history = model.fit(train_inputs, train_labels, epochs=20, batch_size=128, validation_data=(dev_inputs, dev_labels), callbacks=[early_stopping, save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['start_accuracy'], 'g-', label='start_accuracy')\n",
    "plt.plot(history.history['val_start_accuracy'], 'k--', label='val_start_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['end_accuracy'], 'b-', label='end_accuracy')\n",
    "plt.plot(history.history['val_end_accuracy'], 'g--', label='val_end_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-surname",
   "metadata": {},
   "source": [
    "## 4. BERTì˜ ëª¨ë¸ êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤\n",
    "\n",
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation í•¨ìˆ˜\n",
    "    :param x: ì…ë ¥ ê°’\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n",
    "\n",
    "\n",
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer ìƒì„±\n",
    "    :param stddev: ìƒì„±í•  ëœë¤ ë³€ìˆ˜ì˜ í‘œì¤€í¸ì°¨\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer ìƒì„±\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "\n",
    "\n",
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    jsonì„ config í˜•íƒœë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        fileì—ì„œ Configë¥¼ ìƒì„± í•¨\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode == \"embedding\" ì¼ ê²½ìš° Token Embedding Layer ë¡œ ì‚¬ìš©ë˜ëŠ” layer í´ë˜ìŠ¤ì…ë‹ˆë‹¤. \n",
    "\n",
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shared Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        ìƒì„±ì\n",
    "        :param config: Config ê°ì²´\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight ìƒì„±\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer ì‹¤í–‰\n",
    "        :param inputs: ì…ë ¥\n",
    "        :param mode: ì‹¤í–‰ ëª¨ë“œ\n",
    "        :return: embedding or linear ì‹¤í–‰ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        # modeê°€ embeddingì¼ ê²½ìš° embedding lookup ì‹¤í–‰\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # modeê°€ linearì¼ ê²½ìš° linear ì‹¤í–‰\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # modeê°€ ê¸°íƒ€ì¼ ê²½ìš° ì˜¤ë¥˜ ë°œìƒ\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: ì…ë ¥\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear ì‹¤í–‰\n",
    "        :param inputs: ì…ë ¥\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        ìƒì„±ì\n",
    "        :param config: Config ê°ì²´\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer ì‹¤í–‰\n",
    "        :param inputs: ì…ë ¥\n",
    "        :return embed: positional embedding lookup ê²°ê³¼\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        ìƒì„±ì\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer ì‹¤í–‰\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: ì‹¤í–‰ ëª¨ë“œ\n",
    "        :return attn_out: attention ì‹¤í–‰ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        ìƒì„±ì\n",
    "        :param config: Config ê°ì²´\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer ì‹¤í–‰\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: ì‹¤í–‰ ëª¨ë“œ\n",
    "        :return attn_out: attention ì‹¤í–‰ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        ìƒì„±ì\n",
    "        :param config: Config ê°ì²´\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer ì‹¤í–‰\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward ì‹¤í–‰ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        ìƒì„±ì\n",
    "        :param config: Config ê°ì²´\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer ì‹¤í–‰\n",
    "        :param enc_embed: enc_embed ë˜ëŠ” ì´ì „ EncoderLayerì˜ ì¶œë ¥\n",
    "        :param self_mask: enc_tokensì˜ pad mask\n",
    "        :return enc_out: EncoderLayer ì‹¤í–‰ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        ìƒì„±ì\n",
    "        :param config: Config ê°ì²´\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, enc_tokens, segments):\n",
    "        \"\"\"\n",
    "        layer ì‹¤í–‰\n",
    "        :param enc_tokens: encoder tokens\n",
    "        :param segments: token segments\n",
    "        :return logits_cls: CLS ê²°ê³¼ logits\n",
    "        :return logits_lm: LM ê²°ê³¼ logits\n",
    "        \"\"\"\n",
    "        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = enc_out\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: ì…ë ¥ tokens\n",
    "        :param segments: ì…ë ¥ segments\n",
    "        :return embed: embedding ê²°ê³¼\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-martin",
   "metadata": {},
   "source": [
    "## 5. BERT ëª¨ë¸ì„ ì´ìš©í•œ ë„ì „"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4KorQuAD(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(name='BERT4KorQuAD')\n",
    "\n",
    "        self.bert = BERT(config)\n",
    "        self.dense = tf.keras.layers.Dense(2)\n",
    "    \n",
    "    def call(self, enc_tokens, segments):\n",
    "        logits_cls, logits_lm = self.bert(enc_tokens, segments)\n",
    "\n",
    "        hidden = self.dense(logits_lm) # (bs, n_seq, 2)\n",
    "        start_logits, end_logits = tf.split(hidden, 2, axis=-1)  # (bs, n_seq, 1), (bs, n_seq, 1)\n",
    "\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        start_outputs = tf.keras.layers.Softmax(name=\"start\")(start_logits)\n",
    "\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "        end_outputs = tf.keras.layers.Softmax(name=\"end\")(end_logits)\n",
    "\n",
    "        return start_outputs, end_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 384, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_batch_size = 32 \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels)).shuffle(10000).batch(bert_batch_size)\n",
    "dev_dataset = tf.data.Dataset.from_tensor_slices((dev_inputs, dev_labels)).batch(bert_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT4KorQuAD(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, loss_fn, acc_fn, optimizer):\n",
    "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
    "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
    "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
    "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
    "\n",
    "    p_bar = tqdm(dataset)\n",
    "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(p_bar):\n",
    "        with tf.GradientTape() as tape:\n",
    "            start_outputs, end_outputs = model(enc_tokens, segments)\n",
    "\n",
    "            start_loss = loss_fn(start_labels, start_outputs)\n",
    "            end_loss = loss_fn(end_labels, end_outputs)\n",
    "            loss = start_loss + end_loss\n",
    "\n",
    "            start_acc = acc_fn(start_labels, start_outputs)\n",
    "            end_acc = acc_fn(end_labels, end_outputs)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        metric_start_loss(start_loss)\n",
    "        metric_end_loss(end_loss)\n",
    "        metric_start_acc(start_acc)\n",
    "        metric_end_acc(end_acc)\n",
    "        if batch % 10 == 9:\n",
    "            p_bar.set_description(f'loss: {metric_start_loss.result():0.4f}, {metric_end_loss.result():0.4f}, acc: {metric_start_acc.result():0.4f}, {metric_end_acc.result():0.4f}')\n",
    "    p_bar.close()\n",
    "\n",
    "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataset, loss_fn, acc_fn):\n",
    "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
    "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
    "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
    "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
    "\n",
    "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(dataset):\n",
    "        start_outputs, end_outputs = model(enc_tokens, segments)\n",
    "\n",
    "        start_loss = loss_fn(start_labels, start_outputs)\n",
    "        end_loss = loss_fn(end_labels, end_outputs)\n",
    "\n",
    "        start_acc = acc_fn(start_labels, start_outputs)\n",
    "        end_acc = acc_fn(end_labels, end_outputs)\n",
    "\n",
    "        metric_start_loss(start_loss)\n",
    "        metric_end_loss(end_loss)\n",
    "        metric_start_acc(start_acc)\n",
    "        metric_end_acc(end_acc)\n",
    "\n",
    "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "acc_fn = tf.keras.metrics.sparse_categorical_accuracy\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "best_acc = .0\n",
    "patience = 0\n",
    "for epoch in range(20):\n",
    "    train_epoch(model, train_dataset, loss_fn, acc_fn, optimizer)\n",
    "    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, dev_dataset, loss_fn, acc_fn)\n",
    "    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')\n",
    "    acc = start_acc + end_acc\n",
    "    if best_acc < acc:\n",
    "        patience = 0\n",
    "        best_acc = acc\n",
    "        model.save_weights(os.path.join(data_dir, \"korquad_bert_none_pretrain.hdf5\"))\n",
    "        print(f'save best model')\n",
    "    else:\n",
    "        patience += 1\n",
    "    if 5 <= patience:\n",
    "        print(f'early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-hawaiian",
   "metadata": {},
   "source": [
    "# 6. í”„ë¡œì íŠ¸ : Pretrained modelì˜ í™œìš©  \n",
    "ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ pretrained modelì„ í™œìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì‚¬ìš©í•´ì•¼ í•  ëª¨ë¸ êµ¬ì¡°ë‚˜ ë°ì´í„°ì…‹ êµ¬ì¡°, ë°°ì¹˜ êµ¬ì¡°ëŠ” ì´ì „ ìŠ¤í…ê³¼ ë™ì¼í•©ë‹ˆë‹¤. ë‹¤ìŒ ì•ˆë‚´ë¥¼ ë”°ë¼ ì´ë¯¸ ë‹¤ìš´ë¡œë“œí•œ pretrained modelì„ í™œìš©í•˜ëŠ” í•™ìŠµì„ ë‹¤ì‹œ ì§„í–‰í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-hearing",
   "metadata": {},
   "source": [
    "## STEP 1. pretrained model ë¡œë”©í•˜ê¸°  \n",
    "pretrained modelì„ ë¡œë“œí•˜ì—¬ modelì„ ìƒì„±í•˜ëŠ” ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. model êµ¬ì¡°ëŠ” ì´ì „ ìŠ¤í…ê³¼ ë™ì¼í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = os.path.join(model_dir, 'bert_pretrain_32000.hdf5')\n",
    "\n",
    "model = BERT4KorQuAD(config)\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    #  pretrained model ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ ë¨¼ì € ëª¨ë¸ì´ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.\n",
    "    enc_tokens = np.random.randint(0, len(vocab), (4, 10))\n",
    "    segments = np.random.randint(0, 2, (4, 10))\n",
    "    model(enc_tokens, segments)\n",
    "    \n",
    "    # checkpoint íŒŒì¼ë¡œë¶€í„° í•„ìš”í•œ layerë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤. \n",
    "    model.load_weights(os.path.join(model_dir, \"bert_pretrain_32000.hdf5\"), by_name=True)\n",
    "\n",
    "    model.summary()\n",
    "else:\n",
    "    print('NO Pretrained Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-minister",
   "metadata": {},
   "source": [
    "## STEP 2. pretrained model finetune í•˜ê¸°  \n",
    "í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ì½”ë“œë„ ì´ì „ ìŠ¤í…ê³¼ ë™ì¼í•©ë‹ˆë‹¤. ë‹¨ì§€ í•™ìŠµí•´ì•¼ í•  ëª¨ë¸ì´ ëœë¤ ì´ˆê¸°í™”ëœ ê²ƒì´ ì•„ë‹ˆë¼ pretrained modelì„ ë¡œë“œí•œ ê²ƒì¼ ë¿ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-demonstration",
   "metadata": {},
   "source": [
    "## STEP 3. Inference ìˆ˜í–‰í•˜ê¸°  \n",
    "finetune í•™ìŠµì´ ì™„ë£Œëœ modelì„ í™œìš©í•˜ì—¬ ì‹¤ì œ í€´ì¦ˆ í’€ì´ ê²°ê³¼ë¥¼ í™•ì¸í•´ ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict(model, question, context):\n",
    "    \"\"\"\n",
    "    ì…ë ¥ì— ëŒ€í•œ ë‹µë³€ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    :param model: model\n",
    "    :param question: ì…ë ¥ ë¬¸ìì—´\n",
    "    :param context: ì…ë ¥ ë¬¸ìì—´\n",
    "    \"\"\"\n",
    "    q_tokens = vocab.encode_as_pieces(question)[:args.max_query_length]\n",
    "    c_tokens = vocab.encode_as_pieces(context)[:args.max_seq_length - len(q_tokens) - 3]\n",
    "    tokens = ['[CLS]'] + q_tokens + ['[SEP]'] + c_tokens + ['[SEP]']\n",
    "    token_ids = [vocab.piece_to_id(token) for token in tokens]\n",
    "    segments = [0] * (len(q_tokens) + 2) + [1] * (len(c_tokens) + 1)\n",
    "\n",
    "    y_start, y_end = model(np.array([token_ids]), np.array([segments]))\n",
    "    # print(y_start, y_end)\n",
    "    y_start_idx = K.argmax(y_start, axis=-1)[0].numpy()\n",
    "    y_end_idx = K.argmax(y_end, axis=-1)[0].numpy()\n",
    "    answer_tokens = tokens[y_start_idx:y_end_idx + 1]\n",
    "\n",
    "    return vocab.decode_pieces(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")\n",
    "\n",
    "with open(dev_json) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        question = vocab.decode_pieces(data['question'])\n",
    "        context = vocab.decode_pieces(data['context'])\n",
    "        answer = data['answer']\n",
    "        answer_predict = do_predict(model, question, context)\n",
    "        if answer in answer_predict:\n",
    "            print(i)\n",
    "            print(\"ì§ˆë¬¸ : \", question)\n",
    "            print(\"ì§€ë¬¸ : \", context)\n",
    "            print(\"ì •ë‹µ : \", answer)\n",
    "            print(\"ì˜ˆì¸¡ : \", answer_predict, \"\\n\")\n",
    "        if 100 < i:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-facial",
   "metadata": {},
   "source": [
    "## STEP 4. í•™ìŠµ ê²½ê³¼ ì‹œê°í™” ë¹„êµë¶„ì„  \n",
    "pretrained model ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ í•™ìŠµ ìˆ˜í–‰ ê²½ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ë¥¼ ì‹œê°í™”ë¥¼ í¬í•¨í•˜ì—¬ ë¹„êµë¶„ì„ì„ ì§„í–‰í•´ ë´…ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
